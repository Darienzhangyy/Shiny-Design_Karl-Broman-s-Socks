---
title: "Homework 4"
author: "Eclectic Eagle Scouts"
date: "November 12, 2015"
output: pdf_document
---


**Task 1**

The banality of sock laundering does not typically inspire creative data analyses. A tweet from biostatistician Karl Broman, however, fired up nerds the world over. Noting that every single one of the first 11 socks removed from the dryer belonged to a different pair, Broman remarked that this fact had implications for the total number of socks in the wash. One of the more interesting analyses born out of Broman's tweet utilized Approximate Bayesian Computing (ABC) to tackle the ultimate question: How many socks, pairs and singletons, do we expect are in the laundry? Reproducing and extending the original analysis, we have created a Shiny app which (1) allows users dynamic control over the model's priors, (2) graphically displays the prior and posterior distributions, and (3) summarizes the posterior distribution to present the number of socks - in total, in pairs, and singletons.   

*Priors*

The app presents users with an interface allowing a choice of the number of simulations (text input), as well as sliders for selecting the total number of socks pulled and the number of pairs present. By default, these are set to emulate the Broman tweet, with 11 total socks and 0 pairs. 

The interface also provides users with a choice of two priors for each of two quantities: the total number of socks and the proportion of socks in pairs. For the total number of socks, the user may choose either a poisson prior or a negative binomial prior, both of which are appropriate for count data. The poisson prior allows the user to set a hyperparameter, $\lambda$, corresponding to the mean of the poisson distribution; by default $\lambda$ is fixed at 44, but a slider enables the user to easily alter the hyperparameter. Alternatively, the negative binomial prior parameterization used here permits the user to specify two hyperparameters, $\mu$, the mean, and $\sigma$, the standard deviation. These relate to the typical hyperparameters $p$ and $r$ via the equations $p=\mu/\sigma$ and $r=\mu^2/(\sigma^2-\mu)$, respectively. By default, $\mu$ and $\sigma$ are set to 30 and 15, but these, too, can be adjusted by the user.

For the paired proportion of socks, two distributions are available as priors, the beta and the truncated normal. Although the former is a conjugate prior for the negative binomial distribution, the ABC approach renders this superfluous. The beta distribution requires $\alpha$ and $\beta$ hyperparameters, with the mean defined as $\alpha/(\alpha+\beta)$. By default, $\alpha=15$ and $\beta=2$, yielding a mean of approximately 0.88. Importantly, as the aim is to model a proportion, the support of the beta distribution ranges from 0 to 1. The alternative prior, the truncated normal, can similarly be restricted to the [0,1] range, with hyperparameters $\mu$ (mean) and $\sigma$ at default settings of 0.9 and 0.001. 

*Implementation*

To implement ABC, user-specified hyperparameters are used to generate data the total number of socks, `n_socks`, and the proportion of paired socks, `prop_pairs`. From this, prior distributions for the number of paired socks, `n_pairs`, and the number of singletons, `n_odd`, are easily derived. Next, these prior distributions are fed through a generative model which emulates the sock-from-dryer selection process, yielding the number of sock pairs pulled from a theoretical dryer. These simulated sock pairs are then compared to the user-specified 'ground truth', or zero in the Broman example and app default. Examining the prior values which produced the 'correct' data results in distributions over the four quantities of interest, `n_pairs`, `n_odd`, `prop_pairs`, and - most importantly - `n_socks`. Utilizing the default parameterization outlined in the Publishable Stuff blog post, a best guess of 44 socks (19 pairs and 6 singletons) is obtained.

**Task 2**

The second portion of this assignment involves optimizing the code for speed. Despite the use of 'Tiny Data', the ABC approach is notoriously sluggish and inefficient. To obtain a manageable run time, we took two steps. First, we leverage the power of parallel processing in the generation of all simulated priors. This is accomplished by breaking the user-specified number of simulations into 10 sets and simultaneously feeding each to multiple cores via `mclapply`, before unlisting the result. Second, we take a very similar tactic when feeding the priors through the generative model, splitting them into 8 subsets and running `mclapply`.