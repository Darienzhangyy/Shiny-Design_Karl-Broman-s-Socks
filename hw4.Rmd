---
title: "Homework 4"
author: "Eclectic Eagle Scouts"
date: "November 12, 2015"
output: html_document
---


**Task 1**

The banality of sock laundering does not typically inspire creative data analyses. A tweet from biostatistician Karl Broman, however, fired up nerds the world over. Noting that every single one of the first 11 socks removed from the dryer belonged to a different pair, Broman remarked that this fact had implications for the total number of socks in the wash. One of the more interesting analyses born out of Broman's tweet utilized Approximate Bayesian Computing (ABC) to tackle the ultimate question: How many socks, pairs and singletons, do we expect are in the laundry? Reproducing and extending the original analysis, we have created a Shiny app which (1) allows users dynamic control over the model's priors, (2) graphically displays the prior and posterior distributions, and (3) summarizes the posterior distribution to present the number of socks - in total, in pairs, and singletons.   

*Priors*

The app presents users with an interface allowing a choice of the number of simulations (text input), as well as sliders for selecting the total number of socks pulled and the number of pairs present. By default, these are set to emulate the Broman tweet, with 11 total socks and 0 pairs. 

The interface also provides users with a choice of two priors for each of two quantities: the total number of socks and the proportion of socks in pairs. For the total number of socks, the user may choose either a poisson prior or a negative binomial prior, both of which are appropriate for count data. The poisson prior allows the user to set a hyperparameter, $\lambda$, corresponding to the mean of the poisson distribution; by default $\lambda$ is fixed at 44, but a slider enables the user to easily alter the hyperparameter. Alternatively, the negative binomial prior parameterization used here permits the user to specify two hyperparameters, $\mu$, the mean, and $\sigma$, the standard deviation. These relate to the typical hyperparameters $p$ and $r$ via the equations $p=\mu/\sigma$ and $r=\mu^2/(\sigma^2-\mu)$, respectively. By default, $\mu$ and $\sigma$ are set to 30 and 15, but these, too, can be adjusted by the user.

For the paired proportion of socks, two distributions are available as priors, the beta and the truncated normal. Although the former is a conjugate prior for the negative binomial distribution, the ABC approach renders this superfluous. The beta distribution requires $\alpha$ and $\beta$ hyperparameters, with the mean defined as $\alpha/(\alpha+\beta)$. By default, $\alpha=15$ and $\beta=2$, yielding a mean of approximately 0.88. Importantly, as the aim is to model a proportion, the support of the beta distribution ranges from 0 to 1. The alternative prior, the truncated normal, can similarly be restricted to the [0,1] range, with hyperparameters $\mu$ (mean) and $\sigma$ at default settings of 0.9 and 0.001. 

*Implementation*

To implement ABC, user-specified hyperparameters are used to generate data the total number of socks, `n_socks`, and the proportion of paired socks, `prop_pairs`. From this, prior distributions for the number of paired socks, `n_pairs`, and the number of singletons, `n_odd`, are easily derived. Next, these prior distributions are fed through a generative model which emulates the sock-from-dryer selection process, yielding the number of sock pairs pulled from a theoretical dryer. These simulated sock pairs are then compared to the user-specified 'ground truth', or zero in the Broman example and app default. Examining the prior values which produced the 'correct' data results in distributions over the four quantities of interest, `n_pairs`, `n_odd`, `prop_pairs`, and - most importantly - `n_socks`. Utilizing the default parameterization outlined in the Publishable Stuff blog post, a best guess of 44 socks (19 pairs and 6 singletons) is obtained.

**Task 2**

By implementing Approximate Bayesian Computation, we can successfully analyze how total many socks are there. However, large huge number of simulation and computation slow down our analysis. To improve the efficiency, we choose to use parallelization to make our APP speed up.

mclapply is a parallelized version of lapply, it returns a list of the same length as X, each element of which is the result of applying FUN to the corresponding element of X.

```{r, results=FALSE, error=FALSE, warning=FALSE}
library(parallel)
#parallelize number of socks
d_n_sock = unlist(mclapply(1:10,function(x) 
                             rpois(input$n_sims/10, input$total_lambda),
                                     mc.cores = 24))
d_n_sock = unlist(mclapply(1:10, function(x) rnbinom(input$n_sims/10,
                             mu =input$total_mu,
                              size=size1),mc.cores = 24))
#parallelize number of pair of socks
d_prop_pair = unlist(mclapply(1:10, function(x) rbeta(input$n_sims/10,
                                       input$prop_alpha, input$prop_beta),
                                        mc.cores = 24))
d_prop_pair = unlist(mclapply(1:10, function(x) rtruncnorm(input$n_sims/10,
                                       0,1,input$prop_mu,input$prop_sigma),
                                    mc.cores = 24))
```


Also, we split prior into 8 chunks as list l, and then exert mclapply to, which would accelerate the speed of gen_model function. 

```{r, results=FALSE, error=FALSE, warning=FALSE}
#use parallelization to execute function
unlist(mclapply(l, function(x) apply(x,1, function(x) gen_model(x[1],x[2],x[3],x[4])), mc.cores = 8))
```

Then, when we run the App, parallelization makes our App much faster than before.

